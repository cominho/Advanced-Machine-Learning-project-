\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}XGBoost Architecture}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Step-by-Step Construction of XGBoost}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Step 1: Initialize the Model}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Initial model prediction $F_0(x)$}}{4}{}\protected@file@percent }
\newlabel{fig:initial_model}{{1}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Step 2: Compute Residuals}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Computation of residuals at iteration $m$}}{4}{}\protected@file@percent }
\newlabel{fig:compute_residuals}{{2}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Step 3: Fit a New Tree to the Residuals}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Data and feature subsampling in XGBoost}}{5}{}\protected@file@percent }
\newlabel{fig:subsampling}{{3}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Step 4: Update the Model with Learning Rate}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Updating the model with learning rate $\eta $}}{6}{}\protected@file@percent }
\newlabel{fig:model_update}{{4}{6}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Step 5: Repeat the Process}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sequential process of building trees in XGBoost}}{7}{}\protected@file@percent }
\newlabel{fig:xgboost_process}{{5}{7}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Understanding Key Parameters}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Subsample and colsample\_bytree}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Effects of `subsample` and `colsample\_bytree` on tree construction}}{9}{}\protected@file@percent }
\newlabel{fig:parameter_effects}{{6}{9}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Learning Rate and Number of Estimators}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Relationship between learning rate $\eta $ and number of estimators $M$}}{10}{}\protected@file@percent }
\newlabel{fig:learning_rate_vs_estimators}{{7}{10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Difference Between XGBoost and Random Forest in Terms of Estimators}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison between XGBoost and Random Forest}}{11}{}\protected@file@percent }
\newlabel{fig:xgboost_vs_random_forest}{{8}{11}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Algorithm Flowchart}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Flowchart of the XGBoost algorithm}}{12}{}\protected@file@percent }
\newlabel{fig:xgboost_flowchart}{{9}{12}{}{}{}}
\gdef \@abspage@last{12}
